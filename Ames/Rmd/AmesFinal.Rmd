---
title: "AmesHousing"
author: "Andy Heroy, Kito Patterson, Ryan Quincy Paul"
date: "February 13, 2019"
output: html_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction:

For our project, We will estimate how the sale prices of homes in Ames Iowa are affected by various real estate attributes. First, we will utilize multiple linear regression to predict the sale price on many explanatory variables. Then a Two-way ANOVA will be ran on Sale Price and two categorical variables to analyze group variability.


## Data Description:

As stated above, we will use the Ames housing training dataset for our analysis.  This dataset was created by Dean De Cock as a modern alternative to the oudated Boston housing dataset (detail can be found [here at the Kaggle website](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)). The dataset contains 1460 observations and 79 explanatory variables. For our analysis we will be using some of those 79 for analysis as well as a few of our own created features in the prediction.

## Data Preparation

First off, we need to go through and do a little house cleaning with the data.  The following chunks will take you through that initial process of getting the data clean and presentable for future analysis.  


```{r Dataload}

#setwd("C:/Users/andyh/Google Drive/Education/SMU/Courses/DS_6372_Applied_Statistics/project1/Ames/house-prices-advanced-regression-techniques")
train <- read.csv("../Datasets/train.csv", stringsAsFactors = FALSE)
test <- read.csv("../Datasets/test.csv", stringsAsFactors = FALSE)
#As long as we all have the new folder structure, the following should work for all of us without having to change paths or manually moving files...
#train <- read.csv("../Datasets/train.csv", stringsAsFactors = FALSE)
#test <- read.csv("../Datasets/test.csv", stringsAsFactors = FALSE)

```

After combining the test and train datasets, we decided to create a smaller subset in hopes of targeting homes that represent typical home sale transactions in Ames, IA. As a result, we removed all commercial or severely damaged properties while keeping homes sold under "normal" conditions only.


```{r test_train_bind}
train$split <- "train" #Add column to delineate train
test$split <- "test" #Add column to delineate test
test$SalePrice <- NA #Dummy value for empty SalePrice
df <- rbind(train, test) #Append train and test to make data cleanup easier

#Subsets the data to a Sale Condition of "Normal"" and removal of commercial/severly damaged properties.
#This was done to represent a typical homesale.
df2 <- subset(df, SaleCondition=="Normal" | MSZoning !="C (all)" | Functional != "Sev")

```

We further reduced the dataset by excluding rows missing values rather than imputing the missing values. Lot Frontage NA values were converted to 0 and the GarageYrBlt values were converted to inherit the YearBuilt value if the property did indeed have a  garage. Lastly, all missing numeric values were transformed to 0 and all missing character values were changed to "None".


```{r narm_1}
#Remove rows with NA values per column
df3 <- df2[!is.na(df2$Utilities),]
df3 <- df3[!is.na(df3$Exterior1st),]
df3 <- df3[!is.na(df3$Exterior2nd),]
df3 <- df3[!is.na(df3$MasVnrType),] #Removes same rows for MsVnrArea
df3 <- df3[!is.na(df3$Electrical),]
df3 <- df3[!is.na(df3$BsmtFullBath),]
df3 <- df3[!is.na(df3$BsmtHalfBath),]
df3 <- df3[!is.na(df3$SaleType),]

#Check NA's by column
colSums(is.na(df3)) 

#Replace NA with 0 LotFrontage
df3$LotFrontage[is.na(df3$LotFrontage)] <- 0

#Replace NA with YearBuilt for GarageYrBlt 
df3$GarageYrBlt <- ifelse(is.na(df3$GarageYrBlt) & !is.na(df3$GarageType), df3$YearBuilt, df3$GarageYrBlt)


#Transform remaining NA values to "NA"
#Replacing all numeric "NA" values with 0 #
df3_num <- names(df3[,sapply(df3,function(x) {is.numeric(x)})])
df3[,df3_num] <- sapply(df3[,df3_num],function(x){ ifelse(is.na(x),0,x)})


#Replacing all character "NA" values with "None"
df3_char <- names(df3[,sapply(df3,function(x){is.character(x)})])
df3[,df3_char] <- sapply(df3[,df3_char],function(x){ifelse(is.na(x),"None",x)})

```

The final step in our data scrubbing process was to convert all character columns to factor columns. Now that our final dataset is completely clean and uniform, we can confidently move on to the exploratory analysis process.


```{r factor}
#Turn all character columns to factors 
df3[sapply(df3, is.character)] <- lapply(df3[sapply(df3, is.character)], as.factor)

```

## Exploratory Data Analysis

This portion of the analysis, we decided to aggregate some of the variables in order to better represent some of the housing attributes we thought should be examined.  We chose
    * TotalSqFt to represent total indoor area of the house
    * TotalPorchSqFt to represent total porch space.
    * TotalBaths to represent the total bathroom count
   
    

```{r FeatureAddition}
#Added Features for Analysis
df3$TotalSqFt <- (df3$GrLivArea + df3$TotalBsmtSF + df3$GarageArea)
df3$TotalPorchSqFt <- (df3$OpenPorchSF+df3$EnclosedPorch+df3$ScreenPorch)
df3$TotalBaths <- df3$BsmtFullBath+(df3$BsmtHalfBath*0.5)+df3$FullBath+(df3$HalfBath*0.5)
df3$HouseAge <- as.numeric(df3$YrSold) - as.numeric(df3$YearBuilt)

```


Now we'll generate an inital scatterplot matrix to view the untransformed numerical data and see if we find any correlation.  We picked these variables in order to look at our feature creation and seeing how square footage, in all its forms, plays into the Sale Price.


```{r Initial_Scatter_plots}


#Also to check for multi-collinelarity 
#Should we include calculated columns to divide all SF metrics by 100?
pairs(~SalePrice + TotalSqFt + TotalPorchSqFt + HouseAge + 
        LotFrontage + LotArea + MasVnrArea + PoolArea, data=df3, main="Untransformed Scatterplot")

#Different Scatterplot view of the training data.
require(psych)
library(psych)
pairs.panels(df3[df3$split == "train",c("SalePrice", "TotalSqFt", "TotalPorchSqFt","HouseAge","LotFrontage", 
                         "LotArea", "MasVnrArea", "PoolArea")],
             main="Untransformed",
             method = "pearson",
             density = TRUE)


```

While there does appear to be some correlation between the area and age related variables, the data doesn't seem to be normally distributed.  As such, we're going to try a log transformation to see if normality can be improved.


``` {r VariableLogging}

# Logged variables for regression
df3$log_SalePrice <- as.numeric(ifelse(df3$split=="train",log(df3$SalePrice)," "))
df3$log_TotalSqFt <- log(df3$TotalSqFt+1)
df3$log_TotalPorchSqFt <- log(df3$TotalPorchSqFt+1)
df3$log_HouseAge <- log(df3$HouseAge+1)
df3$log_LotFrontage <- log(df3$LotFrontage+1)
df3$log_LotArea <- log(df3$LotArea+1)
df3$log_MasVnrArea <- log(df3$MasVnrArea+1)
df3$log_PoolArea <- log(df3$PoolArea+1)

#Remove columns used to calculate Features below

df3 <- subset(df3, select = -c(Id, GrLivArea, TotalBsmtSF, GarageArea, OpenPorchSF, EnclosedPorch, ScreenPorch, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, YrSold, YearBuilt))

#Split df3 back to training set to use the logged Sale Price and begin considering our model.
split_df <- split(df3, df3$split)
df_train <- split_df[[2]]
df_test <- split_df[[1]]


```


First we'll try a log-linear approach and only log the Sale price. 

```{r Log_Linear_SP, fig.height=10, fig.width=10}
#Scatterplot matrix for log/linear relationship 
pairs(~log_SalePrice + TotalSqFt + TotalPorchSqFt + HouseAge + 
        LotFrontage + LotArea + MasVnrArea + PoolArea, data=df_train, main="Log/Linear Scatterplot")

#Different Scatterplot view
pairs.panels(df_train[,c("log_SalePrice", "TotalSqFt", "TotalPorchSqFt","HouseAge","LotFrontage", 
                         "LotArea", "MasVnrArea", "PoolArea")],
             main="Log/Linear",
             method = "pearson",
             density = TRUE)

```

While we see an improvement in the Log_Sale Price distribution, some of the other features still aren't behaving, so we'll log them all now to see what we can find. 

```{r Log_log, fig.height=10, fig.width=10}
#Scatterplot matrix for log/log relationship 
pairs(~log_SalePrice + log_TotalSqFt + log_TotalPorchSqFt + HouseAge + 
        log_LotFrontage + log_LotArea + log_MasVnrArea + log_PoolArea, data=df_train, main="Log/Log Scatterplot")

#Different Scatterplot view
#Returns -inf NAN values when logging explanatory variables 
pairs.panels(df_train[,c("log_SalePrice", "log_TotalSqFt", "log_TotalPorchSqFt","HouseAge","log_LotFrontage", 
                         "log_LotArea", "log_MasVnrArea", "log_PoolArea")],
             main="Log/Log",
             method = "pearson",
             density = TRUE)

```

Now we see the log-log doesn't necessarily improve normality so we will probably stick with log-linear for a transformation for now.  Next lets load a heat map and look at the correlation matrix.


```{r corHeatMap, fig.height=10, fig.width=10}
#Check for multi-collinelarity
library(corrplot)
#Return numeric values only
df_train_numeric <- df_train[, sapply(df_train, is.numeric)]
#df_train_numeric <- df_train_numeric[,-c(30:36)] #Remove log columns with NaN
#Correlation Plot
df_corr <- round(cor(df_train_numeric),2)
corrplot(df_corr, method="circle", order="hclust", addrect=4, win.asp=.7, title="Variable Corr Heatmap",tl.srt=60)

#Possible multi-collinelarity (Keep TotalSqFt and TotRmsAbvGrd and toss the rest?)
#TotalSqFt vs TotalBaths <br/>
#TotalSqFt vs OverallQual
#TotalSqFt vs X1stFlrSF
#TotalSqFt vs GarageCars
#TotalSqFt vs TotRmsAbvGrd
#TotRmsAbvGrd vs X2ndFlrSF
#TotRmsAbvGrd vs BedroomAbvGrd
#TotRmsAbvGrd vs GarageYrBlt
```

To get a few other views on our dataset



## Model Selection


Now that we've explored a bit more of the data, we're going to look at some other modes of selection to also give us an idea of the order of importance.  

```{r fig1, fig.height=10, fig.width=10}
library(randomForest)
rf_model <- randomForest(SalePrice~., data=df_train, importance = TRUE)
importance(rf_model)
#Variable importance for placement order (Forward, Backward, Stepwise) 
varImpPlot(rf_model,type=1, main='Random Tree Variable Importance')
```



## Objective 1 - Multiple Linear Regression
**NOTE:** Due to slight differences in dataset manipulation when we divied out the tasks and when we put together the parts, the analysis below will not match the results of the code. We still believe that the model we have produced due to the analysis is still a good one across seeds/minor dataset variations.

We will use forward selection to determine the most effective explanatory variables that can be used to predict the sale price of homes in Ames, Iowa. First we will split the `training.csv` given in the Kaggle data set into another training and test set to determine the effectiveness of our models as the `test.csv` in the Kaggle set does not provide sale prices. We will also be dropping factors. Some with only one level, logged variables that we decided above not to use, and Id as it is a known arbitrary field.

A quick check of the assumptions by plotting the full model yields some mixed results. There are some outliers according to the Residuals vs Fitted and Cook's D plots. We believe the analysis can continue with them as they are legitimate houses with high prices. The residuals are relatively normal shaped according to the QQ plot and the samples are assumed to be independent.

```{r setSeedGetTraining, echo=FALSE}
set.seed(25) #Used this to sanity check the ASE results
```

```{r splitTrainingSet}
trainingSet <- df3[df3$split == "train",] # Getting back just the records found in `training.csv`
index<-sample(1:dim(trainingSet)[1], dim(trainingSet)[1] / 2, replace = F) #Used to divide the set into 2 sets

columnsToDrop <- c("Utilities", "SaleCondition","log_TotalBaths","log_TotalSqFt_100","Id","split", "log_TotalSqFt","log_TotalPorchSqFt","log_HouseAge", "log_MasVnrArea","log_LotFrontage","log_PoolArea", "log_SalePrice","log_LotArea")
trainingSet <- trainingSet[, !names(trainingSet) %in% columnsToDrop]

par(mfrow=c(2,2))
plot(lm(log(SalePrice)~.,trainingSet))

mlrHousesTrain<-trainingSet[index,]
mlrHousesTest<-trainingSet[-index,]

#Size of each set
dim(mlrHousesTrain)
dim(mlrHousesTest)
```

Below you will see a few measures used to determine the effectiveness of the model at each step of the forward selection. First is the Bayesian Information Criterion (BIC), a lower value means the predicted values are closer to the actual results. As can be seen at about 40 predictors, the BIC also has a penalty that goes up as the number of predictors goes up. Next is the adjusted R-squared, a measure of how much of the variability is explained by the model, which peaks at about 100 predictors. Finally, we have the residual sum of squares (RSS), the sum of the squared differences of the predictions from the actual results without a penalty like the BIC, which has its lowest point when all the predictors are added.

```{r runningForwardSelection, echo=FALSE}
library(leaps)

#There are 239 variables as it splits categoricals into multiple variables. Setting that as the max. really.big must be true on sets with more than 50 variables
reg.fwd=regsubsets(log(SalePrice)~., data = mlrHousesTrain, method = "forward", really.big=T, nvmax=239)

#Getting total runs for the x-axis of the plots below by counting how many bic values are returned
totalRuns <- length(summary(reg.fwd)$bic)
par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:totalRuns,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

adjr2<-summary(reg.fwd)$adjr2
plot(1:totalRuns,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

rss<-summary(reg.fwd)$rss
plot(1:totalRuns,rss,type="l",ylab="Train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)
```

We will also look at the average squared error (ASE) of the predictions vs the actual values of each step in the feature selection. As can be seen below, and like the RSS above, it continues to decrease until all the variable are added. However, there is a final significant dip at about 60 predictors. 
```{r ASE, echo=FALSE}
#Function provided by Dr. Turner in Unit 2 to give predictions at a specific step of feature selection
predict.regsubsets =function (object , newdata ,id ,...){
  form=as.formula (object$call [[2]])
  mat=model.matrix(form ,newdata )
  coefi=coef(object ,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

testASE<-c()
for (i in 1:totalRuns){
  predictions<-predict.regsubsets(object=reg.fwd,newdata=mlrHousesTest,id=i) 
  testASE[i]<-mean((log(mlrHousesTest$SalePrice)-predictions)^2)
}
par(mfrow=c(1,1))
plot(1:totalRuns,testASE,type="l",xlab="# of predictors",ylab="Test vs Train ASE",ylim=c(0,.5))

lowestASEModelIndex<-which(testASE==min(testASE))
# in case multiple models have the same ASE
if (length(lowestASEModelIndex) > 1) {
  lowestASEModelIndex = lowestASEModelIndex[1]
}
points(index,testASE[lowestASEModelIndex],col="red",pch=10)
rss<-summary(reg.fwd)$rss

testSampleSize <- dim(mlrHousesTest)[1]
lines(1:totalRuns,rss/testSampleSize,lty=3,col="blue")  #Dividing by 100 since ASE=RSS/sample size
```

After more drilling, we've determined that the last significant drop in ASE is found at step 58 where the ASE drops from .04 to .0226. The following fields were selected:

```{r fields}
names(coef(reg.fwd,58))
```

Included are some categorical factors, but not all the levels of those factors. From here we will create a linear model with every continuous variable or categorical variable that is represented by at least 1 level above. The summary of the fit is too long to paste here (see the full list in the Appendix under the Stepwise Model Selection Coefficient Summary heading). Below are some selected regression coefficients and their interpretation. As described earlier, we use a log-linear regression which means we logged the sale price but none of the explanatory variables. Besides the intercept, in the interpretations for the slope coefficients, assume all other variables are held constant.

```{r linearModel}
# I dropped street below because in this set it only had one level. Not enough variation to keep it in.
forwardSelectedModel <- lm(log(SalePrice)~MSZoning+LotFrontage+LotArea+LotConfig+Neighborhood+Condition1+HouseStyle+RoofMatl+Exterior1st+ExterCond+Foundation+BsmtExposure+BsmtFinType1+BsmtFinSF1+BsmtFinType2+Heating+CentralAir+Electrical+LowQualFinSF+TotRmsAbvGrd+Functional+Fireplaces+FireplaceQu+GarageType+GarageFinish+Fence+MoSold+SaleType+TotalSqFt+Condition2+GarageCond+MiscFeature, data = mlrHousesTrain)
```

| Variable | Coefficient | p-value | Interpretation |
| -------  | ----------- | ------ |-------------- |
| Intercept | 11.04 | < 2e-16 | With all variables at 0, the sale price of the house would be $e^{11.04}$ = $62317.65 |
| Total Indoor Area (sqft) | 2.167e-4 | < 2e-16 | A 100 sqft increase would yield a .22% increase in sale price |
| Unfinished Garage | -.0697 | 5.16e-4 | An unfinished garage decreases the sale price by -6.9% |
| Lot Area (sqft) | 1.75e-6 | .0163 | Every acre (43,560 sqft) yields a 7.62% increase in sale price |
| Crawford Neighborhood | .1713 | .00604 | The sale price of a home in the Crawford neighborhood is increased 17.13% |
| MeadowV Neighborhood | -.32 | 1.06e-4 | The sale price of a home in the MeadowV neighborhood is decreased 32% |
| Stucco Exterior | .258 | 8.94e-4 | A stucco exterior increases the sale price 25.8% |
| Asphalt Shingle Exterior Covering | .2263 | .138 | An asphalt shingle exterior covering increases the sale price 22.6% |

Again, the variables above are just a few of the ones included in the model selected by forward stepwise selection. The starting price with all variables at zero is highly significant as well as the increase of price per square foot of indoor space. We also have examples of highly significant and high practical impact categorical levels (Crawford and MeadowV neighborhoods) as well as a non-significant but high impact level (asphalt shingle exterior covering). There are also many 
variables thrown into the model that are non-significant ( p-value > .05). Below I will fit one more "hybrid" model with all non-significant continuous variables and categorical variables without a significant level removed from the forward selection model. It looks like the total indoor square footage was the last highly significant variable added at step 58 and many non-significant factors were added on the way. It's hard to determine the order, as the `regsubset` method will shuffle the variables if it detects high colinearity.

```{r forwardSelectedModelWithoutNonSignificantVars}
#Removed MSZoning,LotFrontage,HouseStyle,Foundation,BsmtFinSF1,Electrical,LowQualFinSF,TotRmsAbvGrd,Fireplaces,FireplaceQu,Fence,MoSold,Condition2,GarageCond,MiscFeature
hybridModel <- lm(log(SalePrice)~LotArea+LotConfig+Neighborhood+Condition1+RoofMatl+Exterior1st+ExterCond+BsmtExposure+BsmtFinType1+BsmtFinType2+Heating+CentralAir+Functional+GarageType+GarageFinish+SaleType+TotalSqFt, data = mlrHousesTrain)
```

The ASE of the new model (compared with .0226 above):
```{r aseOfHybridModel, message=FALSE}
#Removing test set rows levels that were not present in the training set. I should've checked for level representation in both sets with the seed specified. Too late to fix the right way now and change my analysis above. These account for 16 rows and prompts a warning because of prediction on a set with less rows than the one used to make the fit.
mlrHousesTest2 <- mlrHousesTest[!mlrHousesTest$RoofMatl %in% c("ClyTile","Roll","Membran") & mlrHousesTest$Heating != "OthW",]
hybridModelPredictions <- predict(hybridModel, newdata=mlrHousesTest2, interval="confidence") 
hybridModelTestASE <- mean((log(mlrHousesTest2$SalePrice) - hybridModelPredictions[,1])^2) #Selecting the fit column of predictions
hybridModelTestASE
```
This is better than the ASE from the forward model selection and significantly reduces the number of variables in the fit, thereby reducing the risk overfitting. Below are the first five predictions along with the confidence intervals. Since we logged the sale prices, we transform them back to meaningful numbers by raising them as a power of e. Looking at the first prediction we see a prediction of $190,506 with a 95% confidence interval of $183,174 and $198,131.

```{r predictionsWithConfidenceIntervals}
head(exp(hybridModelPredictions))
```

### Conclusion
We believe that we our final model is a good one that separates the wheat from the chaff with regard to relevant factors. As far as new insights, we have learned about the simplicity of feature selection. Non-significant factors are not pruned on the way to the most accurate model. Manual pruning needs to be done after the feature selection has completed to reduce needless complexity. Finally, next time we would like to have a simpler dataset so that small changes in how we put together the test and training sets will not throw off analysis that has already been done as can be seen in the differences between the paragraphs and code output above. 

### Objective 2 - A Two way Anova


Whenever anyone is looking to buy a house, there's always one common metric that comes up.  Total Square feet, which is an added feature but something that always is associated with the buying of a house.  Due to most poeple out there all driving cars or need private maker shops of their own, The second categorical value we chose for a two way anova was GarageType.  We midwesterners are very concerned with our vehicles because if we want to get anywhere its going to take at least a 2 hour car ride to get there.


With looking at various categoricals to pick from, trying to find an even blend of saturation across the group means is our goal.  After experimenting with many.  many. combinations of the 79 categorical variables.  We rested on 

```{r ANOVA}

library(ggplot2)
library(dplyr)

Clean.train <- subset(df3, split=="train")

mysummary<-function(x){
  result<-c(length(x),min(x),max(x),IQR(x),mean(x),sd(x),sd(x)/sqrt(length(x)))
  names(result)<-c("N","Min","Max","IQR","Mean","SD","SE")
  return(result)
}

sumstats<-aggregate(SalePrice~HeatingQC*CentralAir,data=Clean.train,mysummary)
sumstats<-cbind(sumstats[,1:2],sumstats[,-(1:2)])
sumstats


ggplot(sumstats,aes(x=HeatingQC,y=Mean,group=CentralAir,colour=CentralAir))+
  ylab("Sale Price")+
  geom_line()+
  geom_point()+
  geom_errorbar(aes(ymin=Mean-SE,ymax=Mean+SE),width=.1)

#GarageType & GarageFinish

with(Clean.train, table(HeatingQC, CentralAir))


# 
# Clean.train <- Clean.train[!(Clean.train$HeatingQC == "Po"),]
# Clean.train <- subset(Clean.train, HeatingQC != "Po")
Clean.train <- 

head(Clean.train)

levels(Clean.train$HeatingQC)



#Might be a way to do it.  Sort neighborhoods by a particular parameter.  Analyze the top ones.  

# go through these and find the neighborhoods and Housestyles that have most of the each others variables.  Its the single one's that are erroring. 

#reduce it to a set of categoricals that you can work with.  IE.  look through neighborhoods and kick out the bad ones. if you wanna do neighborhood

```

```{r}
with(Clean.train, table(Neighborhood, MSZoning))

ggplot(Clean.train, aes(x = HeatingQC, y = SalePrice)) +
        geom_point(shape=1) + 
        geom_smooth(method=lm, se=FALSE) +   
        xlab("Neighborhood") +
        ylab("Sale Price") +
        theme(text = element_text(size=9)) +
        ggtitle("SalePrice by Neighborhood") 

ggplot(Clean.train, aes(x = CentralAir, y = SalePrice)) +
        geom_point(shape=1) + 
        geom_smooth(method=lm, se=FALSE) +   
        xlab("Building Type") +
        ylab("Sale Price") +
        theme(text = element_text(size=9)) +
        ggtitle("Sale Price by Building Type") 

```

# Now lets take a look at assumptions of the residuals in order to make sure we've satisfied all those before we can proceed with an Anova.


```{r Residual_Plotting}


library(car)
require(gridExtra)
library(gridExtra)
library(grid)
library(ggplot2)

ano.fit <-aov(SalePrice~Neighborhood+HouseStyle+Neighborhood:HouseStyle,data=Clean.train)

table(Clean.Train$Neighborhood,Clean.train$HouseStyle)
# go through these and find the neighborhoods and Housestyles that have most of the each others variables.  Its the single one's that are erroring. 

#reduce it to a set of 

Anova(ano.fit,type=3)

ano.fit <-aov(SalePrice~Neighborhood+GarageType+Neighborhood:GarageType,data=Clean.train)
#Anova(ano.fit,type=3)

with(Clean.train, table(Neighborhood, GarageType))
```

```{r}
please.fit<-aov(SalePrice~Neighborhood+GarageType+Neighborhood:GarageType,data=Clean.train)

plsfit<-data.frame(fitted.values=please.fit$fitted.values,residuals=please.fit$residuals)

#Residual vs Fitted
plot1<-ggplot(plsfit,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
plot2<-ggplot(plsfit,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(plsfit$residuals), slope = sd(plsfit$residuals))

#Histogram of residuals
plot3<-ggplot(plsfit, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),binwidth=10,color="black", fill="gray")+
  geom_density(alpha=.1, fill="red")

grid.arrange(plot1, plot2,plot3, ncol=3)

```


  * Normality - Histogram and QQ Plots
    * The Histogram and QQ plot show some curvature in Q1 and Q3 sections.  A log transform could help straighten that out. 
  * Constant Variance - Residuals plot
  * Independence of variables - We have independence of variables
  * Check for outliers.

  
## Appendix
### Stepwise Model Selection Coefficient Summary
Original Forward Selected Model
```{r appendixForwardSelectionSummary}
summary(forwardSelectedModel)
```

```{r appendixHybridModelSummary}
summary(hybridModel)
```
