---
title: "AmesHousing"
author: "Andy Heroy, Kito Patterson, Ryan Quincy Paul"
date: "February 13, 2019"
output: html_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction:

Due to unforseen previous problems in other dataset choices, we selected the Ames housing dataset in order to complete our assignment.  We apologize for the repetition and unoriginality that come with this choice, but due to time restraints we needed a dataset that was already vetted and ready for analysis.  we hope to make it an enjoyable and efficient analysis experience.  


## Data Description:

As stated above, we will use the Ames housing training dataset for our analysis.  Created by Dean De Cock as a modern alternative to the oudated Boston housing dataset.  The dataset, which was obtained on Kaggle, contains 1460 observations and 79 explanatory variables.   With respect to our analysis, we will be using some of those 79 for analysis, as well as a few of our own created features in the prediction. Many variables influence the sale price of a home. We will estimate the Sale price of the home as it relates too___

### NEED BETTER ENDING ON DATA DESCRIPTION

## Exploratory Data Analaysis

First off, we need to go through and do a little house cleaning with the data.  The following chunks will take you through that initial process of getting the data clean and presentable for future analysis.  

### Will Go through and edit that to better mirror what our EDA ends up being.

```{r Dataload1}
#setwd("C:/Users/andyh/Google Drive/Education/SMU/Courses/DS_6372_Applied_Statistics/project1/Ames/house-prices-advanced-regression-techniques")
#train <- read.csv("train.csv", stringsAsFactors = FALSE)
#test <- read.csv("test.csv", stringsAsFactors = FALSE)
#As long as we all have the new folder structure, the following should work for all of us without having to change paths or manually moving files...
train <- read.csv("../Datasets/train.csv", stringsAsFactors = FALSE)
test <- read.csv("../Datasets/test.csv", stringsAsFactors = FALSE)

library(tidyr)
library(ggplot2)
library(dplyr)

```

## Kito DataCleaning
```{r test_test_train_bind}
train$split <- "train" #Add column to delineate train
test$split <- "test" #Add column to delineate test
test$SalePrice <- NA #Dummy value for empty SalePrice
df <- rbind(train, test) #Append train and test to make data cleanup easier
df2 <- subset(df, SaleCondition=="Normal" & MSZoning !="C (all)" & Functional != "Sev") #Subset to represent typical purchases

```

```{r narm_1}
#Remove rows with NA values per column
#df3 <- df2[!is.na(df2$MSZoning),] #Nothing to remove 
df3 <- df2[!is.na(df2$Utilities),]
df3 <- df3[!is.na(df3$Exterior1st),]
df3 <- df3[!is.na(df3$Exterior2nd),]
df3 <- df3[!is.na(df3$MasVnrType),] #Removes same rows for MsVnrArea
df3 <- df3[!is.na(df3$Electrical),]
df3 <- df3[!is.na(df3$BsmtFullBath),]
df3 <- df3[!is.na(df3$BsmtHalfBath),]
df3 <- df3[!is.na(df3$SaleType),]

#Check NA's by column
colSums(is.na(df3)) 

#Replace NA with 0 LotFrontage
df3$LotFrontage[is.na(df3$LotFrontage)] <- 0

#Replace NA with YearBuilt for GarageYrBlt 
df3$GarageYrBlt <- ifelse(is.na(df3$GarageYrBlt) & !is.na(df3$GarageType), df3$YearBuilt, df3$GarageYrBlt)

```

```{r}
#Transform remaining NA values to "NA"
#df3[is.na(df3)] <- "NA"
#This does some funky stuff to the variable types.  We can't use this and i will go back and change it.  But this for now gets the job done of removing NA's without it messing with the data frame.

# Replacing all numeric "NA" values with 0 #
# Names of all numeric columns
# This changes SalePrice for "test" 0 do we want to leave blank?
df3_num <- names(df3[,sapply(df3,function(x) {is.numeric(x)})])
df3[,df3_num] <- sapply(df3[,df3_num],function(x){ ifelse(is.na(x),0,x)})


# Replacing all character "NA" values with "None"
# Names of all character columns
df3_char <- names(df3[,sapply(df3,function(x){is.character(x)})])
df3[,df3_char] <- sapply(df3[,df3_char],function(x){ifelse(is.na(x),"None",x)})

#Quick Check for NA values.
colSums(is.na(df3)) 

```

```{r}
#Added Features for Analysis
#Wanted to see if the scatterplots look different without the sq ft and log sq ft conversions (/100)

#Should probably remove the individual columns from df once calcs are working
df3$TotalSqFt <- (df3$GrLivArea + df3$TotalBsmtSF + df3$GarageArea)
df3$TotalPorchSqFt <- (df3$OpenPorchSF+df3$EnclosedPorch+df3$ScreenPorch)
#df3$LotArea_100 <- df3$LotArea/100
#df3$MasVnrArea_100 <- df3$MasVnrArea/100
#df3$PoolArea_100 <- df3$PoolArea/100
df3$TotalBaths <- df3$BsmtFullBath+(df3$BsmtHalfBath*0.5)+df3$FullBath+(df3$HalfBath*0.5)
df3$HouseAge <- as.numeric(df3$YrSold) - as.numeric(df3$YearBuilt)

# Logged variables for regression
df3$log_SalePrice <- as.numeric(ifelse(df3$split=="train",log(df3$SalePrice)," "))
df3$log_TotalSqFt <- log(df3$TotalSqFt+1)
df3$log_TotalPorchSqFt <- log(df3$TotalPorchSqFt+1)
df3$log_HouseAge <- log(df3$HouseAge+1)
df3$log_LotFrontage <- log(df3$LotFrontage+1)
df3$log_LotArea <- log(df3$LotArea+1)
df3$log_MasVnrArea <- log(df3$MasVnrArea+1)
df3$log_PoolArea <- log(df3$PoolArea+1)

```

```{r}
#Remove columns used to calculate Features above
#Not sure we need to remove these just yet either.
#Probably should be comfortable removing since they will most likely be correlatated with the calculated variables above.
df3 <- subset(df3, select = -c(Id, GrLivArea, TotalBsmtSF, GarageArea, OpenPorchSF, EnclosedPorch, ScreenPorch, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, YrSold, YearBuilt))
```

```{r}
#Turn all character columns to factors 
df3[sapply(df3, is.character)] <- lapply(df3[sapply(df3, is.character)], as.factor)
#Another way of doing above
#library(Dplyr)
#df3 <- df %>% mutate_if(is.character,as.factor)
```

```{r}
#Split df3 back to train and test df 
split_df <- split(df3, df3$split)
df_train <- split_df[[2]]
df_test <- split_df[[1]]

df_train
df_test
```

```{r fig1, fig.height=10, fig.width=10}
#Initial scatterplot matrix to determine if transformations are needed to create linear relationships 
#Also to check for multi-collinelarity 
#Should we include calculated columns to divide all SF metrics by 100?
pairs(~SalePrice + TotalSqFt + TotalPorchSqFt + HouseAge + 
        LotFrontage + LotArea + MasVnrArea + PoolArea, data=df_train, main="Untransformed Scatterplot")

#Different Scatterplot view
require(psych)
library(psych)
pairs.panels(df_train[,c("SalePrice", "TotalSqFt", "TotalPorchSqFt","HouseAge","LotFrontage", 
                         "LotArea", "MasVnrArea", "PoolArea")],
             main="Untransformed",
             method = "pearson",
             density = TRUE)

```

```{r fig1, fig.height=10, fig.width=10}
#Scatterplot matrix for log/linear relationship 
pairs(~log_SalePrice + TotalSqFt + TotalPorchSqFt + HouseAge + 
        LotFrontage + LotArea + MasVnrArea + PoolArea, data=df_train, main="Log/Linear Scatterplot")

#Different Scatterplot view
pairs.panels(df_train[,c("log_SalePrice", "TotalSqFt", "TotalPorchSqFt","HouseAge","LotFrontage", 
                         "LotArea", "MasVnrArea", "PoolArea")],
             main="Log/Linear",
             method = "pearson",
             density = TRUE)

```

```{r fig1, fig.height=10, fig.width=10}
#Scatterplot matrix for log/log relationship 
pairs(~log_SalePrice + log_TotalSqFt + log_TotalPorchSqFt + HouseAge + 
        log_LotFrontage + log_LotArea + log_MasVnrArea + log_PoolArea, data=df_train, main="Log/Log Scatterplot")

#Different Scatterplot view
#Returns -inf NAN values when logging explanatory variables 
pairs.panels(df_train[,c("log_SalePrice", "log_TotalSqFt", "log_TotalPorchSqFt","HouseAge","log_LotFrontage", 
                         "log_LotArea", "log_MasVnrArea", "log_PoolArea")],
             main="Log/Log",
             method = "pearson",
             density = TRUE)

```

```{r fig1, fig.height=10, fig.width=10}
#Check for multi-collinelarity
library(corrplot)
#Return numeric values only
df_train_numeric <- df_train[, sapply(df_train, is.numeric)]
#df_train_numeric <- df_train_numeric[,-c(30:36)] #Remove log columns with NaN
#Correlation Plot
df_corr <- round(cor(df_train_numeric),2)
corrplot(df_corr, method="circle", order="hclust", addrect=4, win.asp=.7, title="Variable Corr Heatmap",tl.srt=60)

#Possible multi-collinelarity (Keep TotalSqFt and TotRmsAbvGrd and toss the rest?)
#TotalSqFt vs TotalBaths
#TotalSqFt vs OverallQual
#TotalSqFt vs X1stFlrSF
#TotalSqFt vs GarageCars
#TotalSqFt vs TotRmsAbvGrd
#TotRmsAbvGrd vs X2ndFlrSF
#TotRmsAbvGrd vs BedroomAbvGrd
#TotRmsAbvGrd vs GarageYrBlt
```

```{r}
library(Hmisc)
#Correlation and P-value table
flat_cor_mat <- function(cor_r, cor_p){
  #This function provides a simple formatting of a correlation matrix
  #into a table with 4 columns containing :
  # Column 1 : row names (variable 1 for the correlation test)
  # Column 2 : column names (variable 2 for the correlation test)
  # Column 3 : the correlation coefficients
  # Column 4 : the p-values of the correlations
  library(tidyr)
  library(tibble)
  cor_r <- rownames_to_column(as.data.frame(cor_r), var = "row")
  cor_r <- gather(cor_r, column, cor, -1)
  cor_p <- rownames_to_column(as.data.frame(cor_p), var = "row")
  cor_p <- gather(cor_p, column, p, -1)
  cor_p_matrix <- left_join(cor_r, cor_p, by = c("row", "column"))
  cor_p_matrix
}
cor_3 <- rcorr(as.matrix(df_train_numeric))
my_cor_matrix <- flat_cor_mat(cor_3$r, cor_3$P)
head(my_cor_matrix, 5)

``` 

```{r}
library(randomForest)
rf_model <- randomForest(SalePrice~., data=df_train, importance = TRUE)
rf_model
importance(rf_model)
```

```{r fig1, fig.height=10, fig.width=10}
#Variable importance for placement order (Forward, Backward, Stepwise) 
varImpPlot(rf_model,type=1, main='Random Tree Variable Importance')
```






```{r}
Dataholes <- sapply(df3, function(x) sum(is.na(x)))
FirstFocus <- data.frame(index = names(df3), BadData = Dataholes)
FirstFocus[FirstFocus$BadData > 0,]

```


```{r MLRSection}

set.seed(1234) #not sure what protocol is for set.seed, we did this so our homeworks would match, but we probably want this to be actually random?


# Getting back just the training set from massaged data
trainingSet <- df3[df3$split == "train",]

#there are 1193 observations. Divide in half
index<-sample(1:dim(trainingSet)[1], dim(trainingSet)[1] / 2, replace = F)

# dropping Utilities and SaleCondition (we filtered for only Normal) because they only have 1 level
columnsToDrop <- c("Utilities", "SaleCondition","log_TotalBaths","TotalSqFt_100","Id","split")
trainingSet <- trainingSet[, !names(trainingSet) %in% columnsToDrop]

#removing for high colinearity
#highVifColumns <- c("MSSubClass","LotArea","OverallQual","YearRemodAdd","BsmtFinSF1","BsmtFinSF2","BsmtUnfSF","X1stFlrSF","X2ndFlrSF","KitchenAbvGr","TotRmsAbvGrd","Fireplaces","GarageCars","PoolArea","MiscVal","HouseAge","GarageYrBlt","Exterior1st","Exterior2nd","Condition1","Condition2","ExterQual","ExterCond")

#trainingSet <- trainingSet[, !names(trainingSet) %in% c(columnsToDrop,highVifColumns)]


#columnsToInclude <- c("LotArea","MasVnrArea","BsmtFinSF1","Fireplaces","X1stFlrSF","log_TotalSqFt_100","WoodDeckSF","YearRemodAdd","TotalBaths","OverallQual","GarageCars","SalePrice")
#trainingSet <- trainingSet[, names(trainingSet) %in% columnsToInclude]

#splitting into train and test, since the test we are given doesn't have the actual values
mlrHousesTrain<-trainingSet[index,]
mlrHousesTest<-trainingSet[-index,]

library(leaps)


reg.fwd=regsubsets(log(SalePrice)~., data = mlrHousesTrain, method = "forward", nvmax = 50)

summary(reg.fwd)$adjr2
summary(reg.fwd)$rss
summary(reg.fwd)$bic


par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:51,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

adjr2<-summary(reg.fwd)$adjr2
plot(1:51,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

rss<-summary(reg.fwd)$rss
plot(1:51,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)

predict.regsubsets =function (object , newdata ,id ,...){
  form=as.formula (object$call [[2]])
  mat=model.matrix(form ,newdata )
  coefi=coef(object ,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

testASE<-c()
#note my index is to 20 since that what I set it in regsubsets
for (i in 1:51){
  predictions<-predict.regsubsets(object=reg.fwd,newdata=mlrHousesTest,id=i) 
  testASE[i]<-mean((log(mlrHousesTest$SalePrice)-predictions)^2)
}
par(mfrow=c(1,1))
plot(1:51,testASE,type="l",xlab="# of predictors",ylab="test vs train ASE",ylim=c(0,1))
index<-which(testASE==min(testASE))
points(index,testASE[index],col="red",pch=10)
rss<-summary(reg.fwd)$rss

testSampleSize <- dim(mlrHousesTest)[1]
lines(1:51,rss/testSampleSize,lty=3,col="blue")  #Dividing by 100 since ASE=RSS/sample size
```


### Objective 2 - A Two way Anova


Whenever anyone is looking to buy a house, there's always one common metric that comes up.  Total Square feet, which is an added feature but something that always is associated with the buying of a house.  Due to most poeple out there all driving cars or need private maker shops of their own, The second categorical value we chose for a two way anova was GarageType.  We midwesterners are very concerned with our vehicles because if we want to get anywhere its going to take at least a 2 hour car ride to get there.

```{r ANOVA}

library(ggplot2)
library(corrplot)
library(dplyr)


Clean.train <- subset(df3, split=="train")

ggplot(Clean.train, aes(x = TotalSqFt, y = SalePrice)) +
        geom_point(shape=1) + 
        geom_smooth(method=lm, se=FALSE) +   
        xlab("Total Square Footage") +
        ylab("Sale Price") +
        theme(text = element_text(size=9)) +
        ggtitle("Total Sq Foot") 

ggplot(Clean.train, aes(x = GarageType, y = SalePrice)) +
        geom_point(shape=1) + 
        geom_smooth(method=lm, se=FALSE) +   
        xlab("Garage Type") +
        ylab("Sale Price") +
        theme(text = element_text(size=9)) +
        ggtitle("Garage Type") 


ano.fit <-aov(log(SalePrice)~TotalSqFt+GarageType+TotalSqFt:GarageType,data=Clean.train)

library(car)
Anova(ano.fit,type=3)


```



```{r Residual_Plotting, fig.height=4}
require(gridExtra)
library(gridExtra)


myfits<-data.frame(fitted.values=ano.fit$fitted.values,residuals=ano.fit$residuals)

#Residual vs Fitted
plot1<-ggplot(myfits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
plot2<-ggplot(myfits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(myfits$residuals), slope = sd(myfits$residuals))


#Histogram of residuals
plot3<-ggplot(myfits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),binwidth=1,color="black", fill="gray")+
  geom_density(alpha=.1, fill="red")

grid.arrange(plot1, plot2,plot3, ncol=3)

```
