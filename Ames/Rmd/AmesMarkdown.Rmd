---
title: "AmesHousing"
author: "Andy Heroy, Kito Patterson, Ryan Quincy Paul"
date: "February 13, 2019"
output: html_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction:

Due to unforseen previous problems in other dataset choices, we selected the Ames housing dataset in order to complete our assignment.  We apologize for the repetition and unoriginality that come with this choice, but due to time restraints we needed a dataset that was already vetted and ready for analysis.  we hope to make it an enjoyable and efficient analysis experience.  


## Data Description:

As stated above, we will use the Ames housing training dataset for our analysis.  Created by Dean De Cock as a modern alternative to the oudated Boston housing dataset.  The dataset, which was obtained on Kaggle, contains 1460 observations and 79 explanatory variables.   With respect to our analysis, we will be using some of those 79 for analysis, as well as a few of our own created features in the prediction. Many variables influence the sale price of a home. We will estimate the Sale price of the home as it relates too___

### NEED BETTER ENDING ON DATA DESCRIPTION

## Exploratory Data Analaysis

First off, we need to go through and do a little house cleaning with the data.  The following chunks will take you that initial process of getting the data clean and presentable for future analysis.  

### Will Go through and edit that to better mirror what our EDA ends up being.

```{r Dataload}
train <- read.csv("../Datasets/train.csv", stringsAsFactors = FALSE)
test <- read.csv("../Datasets/test.csv", stringsAsFactors = FALSE)
library(tidyr)
library(ggplot2)
library(dplyr)

```

## Kito DataCleaning
```{r test_test_train_bind}
train$split <- "train" #Add column to delineate train
test$split <- "test" #Add column to delineate test
test$SalePrice <- NA #Dummy value for empty SalePrice
df <- rbind(train, test) #Append train and test to make data cleanup easier
df2 <- subset(df, SaleCondition=="Normal") #Filter df on normal house sales

```


```{r narm_1}
#Remove rows with NA values per column
df3 <- df2[!is.na(df2$MSZoning),]
df3 <- df3[!is.na(df3$Utilities),]
df3 <- df3[!is.na(df3$Exterior1st),]
df3 <- df3[!is.na(df3$Exterior2nd),]
df3 <- df3[!is.na(df3$MasVnrType),] #Removes same rows for MsVnrArea
df3 <- df3[!is.na(df3$Electrical),]
df3 <- df3[!is.na(df3$BsmtFullBath),]
df3 <- df3[!is.na(df3$BsmtHalfBath),]
df3 <- df3[!is.na(df3$SaleType),]

#Check NA's by column
colSums(is.na(df3)) 

#Replace NA with 0 LotFrontage
df3$LotFrontage[is.na(df3$LotFrontage)] <- 0
#Replace NA with blanks GarageYrBlt
df3$GarageYrBlt[is.na(df3$GarageYrBlt)] <- " "

```


```{r}
#Transform reamining NA values to "NA"
#df3[is.na(df3)] <- "NA"
#This does some funky stuff to the variable types.  We can't use this and i will go back and change it.  But this for now gets the job done of removing NA's without it messing with the data frame.

# Replacing all numeric "NA" values with 0 #
# Names of all numeric columns
df3_num <- names(df3[,sapply(df3,function(x) {is.numeric(x)})])
df3[,df3_num] <- sapply(df3[,df3_num],function(x){ ifelse(is.na(x),0,x)})


# Replacing all character "NA" values with "None"
# Names of all character columns
df3_char <- names(df3[,sapply(df3,function(x){is.character(x)})])
df3[,df3_char] <- sapply(df3[,df3_char],function(x){ifelse(is.na(x),"None",x)})

#Quick Check for NA values.
colSums(is.na(df3)) 


```

```{r}
#Added Features for Analysis

#Should probably remove the individual columns from df once calcs are working
df3$TotalSqFt_100 <- (df3$GrLivArea + df3$TotalBsmtSF + df3$GarageArea)/100
df3$TotalPorchSqFt_100 <- (df3$OpenPorchSF+df3$EnclosedPorch+df3$ScreenPorch)/100
df3$TotalBaths <- df3$BsmtFullBath+(df3$BsmtHalfBath*0.5)+df3$FullBath+(df3$HalfBath*0.5)
df3$HouseAge <- as.numeric(df3$YrSold) - as.numeric(df3$YearBuilt)

# Logged variables for regression
#df3$log_SalePrice <- log(df3$SalePrice)
df3$log_TotalSqFt_100 <- log(df3$TotalSqFt_100)
df3$log_TotalBaths <- log(df3$TotalBaths)


```

```{r}
#Remove columns used to calculate Features above
#Not sure we need to remove these just yet either.
df3 <- subset(df3, select = -c(GrLivArea, TotalBsmtSF, GarageArea, OpenPorchSF, EnclosedPorch, ScreenPorch, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, YrSold, YearBuilt))
```

```{r}
#Turn all character columns to factors 
df3[sapply(df3, is.character)] <- lapply(df3[sapply(df3, is.character)], as.factor)
#Another way of doing above
#df3 <- df %>% mutate_if(is.character,as.factor)
```

```{r}
#Initial scatterplot matrix to determine if transformations are needed to create linear relationships 
#Also to check for multi-collinelarity 
pairs(~SalePrice + TotalSqFt_100 + TotalPorchSqFt_100 + HouseAge + LotArea + 
        LotFrontage + LotArea + MasVnrArea + PoolArea, data=df3, main="Initial Scatterplot")
```
```{r fig1, fig.height=10, fig.width=10}
#Check for multi-collinelarity
library(corrplot)
#Return numeric values only
df3_numeric <- df3[, sapply(df3, is.numeric)]
#Correlation Plot
df_corr <- round(cor(df3_numeric),2)


corrplot(df_corr, method="circle", order="hclust", addrect=4, win.asp=.7, title="Variable Corr Heatmap",tl.srt=60)
```

```{r}
library(Hmisc)
#Correlation and P-value table
flat_cor_mat <- function(cor_r, cor_p){
  #This function provides a simple formatting of a correlation matrix
  #into a table with 4 columns containing :
  # Column 1 : row names (variable 1 for the correlation test)
  # Column 2 : column names (variable 2 for the correlation test)
  # Column 3 : the correlation coefficients
  # Column 4 : the p-values of the correlations
  library(tidyr)
  library(tibble)
  cor_r <- rownames_to_column(as.data.frame(cor_r), var = "row")
  cor_r <- gather(cor_r, column, cor, -1)
  cor_p <- rownames_to_column(as.data.frame(cor_p), var = "row")
  cor_p <- gather(cor_p, column, p, -1)
  cor_p_matrix <- left_join(cor_r, cor_p, by = c("row", "column"))
  cor_p_matrix
}
cor_3 <- rcorr(as.matrix(df3_numeric))
my_cor_matrix <- flat_cor_mat(cor_3$r, cor_3$P)
my_cor_matrix

``` 



```{r}
Dataholes <- sapply(df3, function(x) sum(is.na(x)))
FirstFocus <- data.frame(index = names(df3), BadData = Dataholes)
FirstFocus[FirstFocus$BadData > 0,]

```


```{r MLRSection}

set.seed(1234) #not sure what protocol is for set.seed, we did this so our homeworks would match, but we probably want this to be actually random?


# Getting back just the training set from massaged data
trainingSet <- df3[df3$split == "train",]

#there are 1193 observations. Divide in half
index<-sample(1:dim(trainingSet)[1], dim(trainingSet)[1] / 2, replace = F)

# dropping Utilities and SaleCondition (we filtered for only Normal) because they only have 1 level
columnsToDrop <- c("Utilities", "SaleCondition","log_TotalBaths","TotalSqFt_100","Id","split")
trainingSet <- trainingSet[, !names(trainingSet) %in% columnsToDrop]

#removing for high colinearity
#highVifColumns <- c("MSSubClass","LotArea","OverallQual","YearRemodAdd","BsmtFinSF1","BsmtFinSF2","BsmtUnfSF","X1stFlrSF","X2ndFlrSF","KitchenAbvGr","TotRmsAbvGrd","Fireplaces","GarageCars","PoolArea","MiscVal","HouseAge","GarageYrBlt","Exterior1st","Exterior2nd","Condition1","Condition2","ExterQual","ExterCond")

#trainingSet <- trainingSet[, !names(trainingSet) %in% c(columnsToDrop,highVifColumns)]


#columnsToInclude <- c("LotArea","MasVnrArea","BsmtFinSF1","Fireplaces","X1stFlrSF","log_TotalSqFt_100","WoodDeckSF","YearRemodAdd","TotalBaths","OverallQual","GarageCars","SalePrice")
#trainingSet <- trainingSet[, names(trainingSet) %in% columnsToInclude]

#splitting into train and test, since the test we are given doesn't have the actual values
mlrHousesTrain<-trainingSet[index,]
mlrHousesTest<-trainingSet[-index,]

library(leaps)


reg.fwd=regsubsets(log(SalePrice)~., data = mlrHousesTrain, method = "forward", nvmax = 50)

summary(reg.fwd)$adjr2
summary(reg.fwd)$rss
summary(reg.fwd)$bic


par(mfrow=c(1,3))
bics<-summary(reg.fwd)$bic
plot(1:51,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

adjr2<-summary(reg.fwd)$adjr2
plot(1:51,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

rss<-summary(reg.fwd)$rss
plot(1:51,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)

predict.regsubsets =function (object , newdata ,id ,...){
  form=as.formula (object$call [[2]])
  mat=model.matrix(form ,newdata )
  coefi=coef(object ,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}

testASE<-c()
#note my index is to 20 since that what I set it in regsubsets
for (i in 1:51){
  predictions<-predict.regsubsets(object=reg.fwd,newdata=mlrHousesTest,id=i) 
  testASE[i]<-mean((log(mlrHousesTest$SalePrice)-predictions)^2)
}
par(mfrow=c(1,1))
plot(1:51,testASE,type="l",xlab="# of predictors",ylab="test vs train ASE",ylim=c(0,1))
index<-which(testASE==min(testASE))
points(index,testASE[index],col="red",pch=10)
rss<-summary(reg.fwd)$rss

testSampleSize <- dim(mlrHousesTest)[1]
lines(1:51,rss/testSampleSize,lty=3,col="blue")  #Dividing by 100 since ASE=RSS/sample size
```




### Objective 2 - A Two way Anova


```{r ANOVA}

library(ggplot2)
library(corrplot)
library(dplyr)

#First lets just plot the GrLivArea by sale price to get an initial feel for the data
#ggplot(df3, aes(x=GrLivArea, y=SalePrice))+ geom_point()
#We might want to ax those four houses that could be causing leverage as they're so far out there.  We will keep note going forward.


```


